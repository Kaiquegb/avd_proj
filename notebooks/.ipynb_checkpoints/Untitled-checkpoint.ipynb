{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516cfd0a-4a6c-41e4-bb5c-0cfba9cdd83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 22:22:45 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/03 22:22:45 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025-12-03 22:22:45 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "2025-12-03 22:22:45 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025-12-03 22:22:45 INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2025-12-03 22:22:46 INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2025-12-03 22:22:46 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "2025-12-03 22:22:46 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2025-12-03 22:22:46 INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2025-12-03 22:22:47 INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2025-12-03 22:22:48 INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2025-12-03 22:22:49 INFO  [alembic.runtime.migration] Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2025-12-03 22:22:50 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "2025-12-03 22:22:50 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/12/03 22:22:50 INFO mlflow.tracking.fluent: Experiment with name 'Agrupamento_Estacoes_PE' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['temperatura', 'umidade', 'vento_velocidade', 'precipitacao'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m     23\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperatura\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mumidade\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvento_velocidade\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitacao\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_profile\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# K-Means\u001b[39;00m\n\u001b[1;32m     27\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3901\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3902\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['temperatura', 'umidade', 'vento_velocidade', 'precipitacao'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Configurar MLFlow (Modo Local)\n",
    "# ---------------------------------------------------------\n",
    "# Usamos SQLite local para garantir que funcione sem erros de rede (403/Forbidden)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"Agrupamento_Estacoes_PE\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Carregar Dados do Banco\n",
    "# ---------------------------------------------------------\n",
    "# Conexão com o container do Postgres\n",
    "engine = create_engine(\"postgresql://admin:admin@postgres:5432/weather_db\")\n",
    "\n",
    "# Lemos a VIEW que você criou no schema.sql.\n",
    "# Ela já traz as médias diárias calculadas, facilitando o trabalho do Python.\n",
    "df_daily = pd.read_sql(\"SELECT * FROM view_medias_diarias\", engine)\n",
    "\n",
    "print(\"Dados carregados do banco:\")\n",
    "print(df_daily.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Criar Perfil da Estação (Média Histórica)\n",
    "# ---------------------------------------------------------\n",
    "# O Tópico 7.4 pede para agrupar as estações baseando-se no comportamento geral delas.\n",
    "# Por isso, tiramos a média de todo o período para cada estação.\n",
    "\n",
    "# Definimos explicitamente as colunas numéricas para evitar erro com 'data_dia'\n",
    "colunas_analise = ['temperatura', 'umidade', 'vento_velocidade', 'precipitacao']\n",
    "\n",
    "# Agrupa por estação e calcula a média das variáveis climáticas\n",
    "df_profile = df_daily.groupby('codigo_estacao')[colunas_analise].mean().reset_index()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Clustering (K-Means) com Registro no MLFlow\n",
    "# ---------------------------------------------------------\n",
    "with mlflow.start_run():\n",
    "    # A. Normalização (Essencial para o K-Means não dar peso errado)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df_profile[colunas_analise])\n",
    "    \n",
    "    # B. Rodar o K-Means\n",
    "    # k=3 (Tentando separar em: Litoral, Agreste, Sertão)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    df_profile['cluster_id'] = kmeans.fit_predict(X)\n",
    "    \n",
    "    # C. Registrar no MLFlow (Requisito do projeto)\n",
    "    mlflow.log_param(\"k\", 3)\n",
    "    mlflow.log_metric(\"inertia\", kmeans.inertia_)\n",
    "    \n",
    "    # Salva o modelo virtualmente\n",
    "    mlflow.sklearn.log_model(kmeans, \"modelo_clustering_clima\")\n",
    "    \n",
    "    print(\"\\nModelo K-Means treinado e registrado no MLFlow com sucesso!\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Visualização (Mapa Estático para o Relatório)\n",
    "# ---------------------------------------------------------\n",
    "# Como o banco não tem Lat/Lon, usamos um dicionário fixo das estações de PE\n",
    "coords = {\n",
    "    'A001_RECIFE':    [-8.05, -34.88], \n",
    "    'A002_PETROLINA': [-9.39, -40.50],\n",
    "    'A003_CARUARU':   [-8.28, -35.97], \n",
    "    'A004_GARANHUNS': [-8.89, -36.49],\n",
    "    'A005_ARARIPINA': [-7.57, -40.49]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Cores para diferenciar os grupos\n",
    "colors = {0: 'red', 1: 'blue', 2: 'green'}\n",
    "\n",
    "# Plotar cada estação no mapa\n",
    "for idx, row in df_profile.iterrows():\n",
    "    estacao = row['codigo_estacao']\n",
    "    cluster = row['cluster_id']\n",
    "    \n",
    "    # Pega lat/lon do dicionário (ou usa 0,0 se não achar)\n",
    "    lat, lon = coords.get(estacao, [0, 0])\n",
    "    \n",
    "    plt.scatter(lon, lat, c=colors.get(cluster, 'gray'), s=300, edgecolors='black', label=f\"Cluster {cluster}\")\n",
    "    plt.text(lon, lat + 0.15, estacao, fontsize=9, ha='center', fontweight='bold')\n",
    "\n",
    "# Ajustes visuais do gráfico\n",
    "plt.title(\"Tópico 7.4: Agrupamento de Estações Meteorológicas (PE)\", fontsize=14)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Remove legendas duplicadas\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), title=\"Grupos Climáticos\")\n",
    "\n",
    "# Salvar a imagem para colocar no relatório\n",
    "plt.savefig('mapa_clusters_final.png')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Exibir Tabela Final (Evidence)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- Resultado do Agrupamento ---\")\n",
    "print(df_profile[['codigo_estacao', 'cluster_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee6053-935e-4818-ac52-258880f294fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
